# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This repository contains deployment scripts for a Multi-Ollama Web Interface - a web application that allows users to interact with multiple Ollama LLM instances simultaneously and compare their responses. The system includes chat history, file uploads, web search capabilities, and consensus generation.

## Commands

### Deployment Commands
- `bash test.sh` - Main deployment script (latest version, v13)
- `bash gemini_deploy.sh` - Alternative deployment script (FINAL v2) 
- `bash grok_deploy.sh` - Another deployment variant (v13, fixed syntax)
- `bash claude_deploy.sh.txt` - Deployment script (v11, file handling focused)

All deployment scripts:
- Install Node.js, npm, sqlite3, and Ollama dependencies
- Create a complete web server with Express.js backend
- Set up systemd service for auto-start
- Configure host persistence and chat history storage
- Deploy on port 3000 by default

### Service Management (after deployment)
- `sudo systemctl status multi-ollama` - Check service status
- `sudo systemctl restart multi-ollama` - Restart the service
- `sudo systemctl stop multi-ollama` - Stop the service
- `sudo journalctl -u multi-ollama -f` - View service logs

### Application Commands (generated by deployment)
- `node server.js` - Start the server manually
- `npm start` - Start via npm script
- Access at `http://localhost:3000` after deployment

## Architecture

### Deployment Scripts Architecture
The repository contains multiple versions of deployment scripts that create a complete web application:

1. **Script Evolution**: Scripts range from v11 to v13, with each version fixing bugs and adding features
2. **Target Environment**: Primarily designed for Arch Linux but includes fallbacks for other distros
3. **Installation Pattern**: All scripts follow the same pattern:
   - Check/install prerequisites (Node.js, npm, sqlite3, Ollama)
   - Create installation directory (`~/multi-ollama-server`)  
   - Generate complete source code (package.json, server.js, public/index.html)
   - Install npm dependencies
   - Create and start systemd service

### Generated Application Architecture
The deployment scripts create a full-stack web application with:

**Backend (Node.js/Express)**:
- **Host Management**: Dynamic addition/removal of Ollama instances across network
- **Model Discovery**: Automatic detection of available models from each host
- **Chat System**: Session-based chat with SQLite persistence
- **File Processing**: Support for text files, code files, and images (up to 50MB)
- **Web Search**: DuckDuckGo integration for internet-enabled responses  
- **Consensus Generation**: Synthesis of multiple model responses
- **Settings Management**: Configurable system prompts stored in SQLite/YAML

**Frontend (Vanilla HTML/CSS/JS)**:
- **Dark Theme Interface**: Professional chat interface with sidebar navigation
- **Real-time Communication**: Direct REST API calls to backend
- **Multi-Model Selection**: Checkbox interface for selecting active models
- **File Upload**: Drag-and-drop file attachment with progress indication
- **Session Management**: Dropdown for loading previous chat sessions
- **Markdown Rendering**: Full markdown support with syntax highlighting

**Data Layer**:
- **SQLite Database**: Chat history, settings, user sessions
- **File Storage**: JSON for host configuration, YAML for settings
- **Temporary Uploads**: Processed and cleaned up automatically

### Key Components

**Host Management System**:
- Supports multiple Ollama endpoints (default: localhost:11434)
- Health checking with online/offline status tracking  
- Persistent configuration in `hosts.json`
- Network discovery for 10.1.10.0/24 subnet

**Multi-Model Response System**:
- Parallel request processing to selected models
- Error handling with fallback to older Ollama API endpoints
- Response aggregation and display with source attribution
- Automatic consensus generation when 2+ models respond

**File Processing Pipeline**:
- Text extraction from code files, documents, and plain text
- Image processing with base64 encoding for vision models
- Size limits and type validation
- Temporary file cleanup

## Important Implementation Notes

### Security Considerations
- File uploads are processed and temporary files cleaned up immediately
- No authentication system implemented - intended for local/trusted network use
- System prompts and settings stored in plaintext database

### Deployment Requirements  
- Requires sudo access for systemd service creation
- Creates system service that auto-starts on boot
- Installs global dependencies (Node.js, Ollama) via package manager

### Network Configuration
Scripts are designed for environments where:
- Local Ollama instance runs on port 11434
- Remote Ollama instances discoverable on 10.1.10.0/24 network  
- Web interface accessible on port 3000

### File Structure (Post-Deployment)
The deployment creates this structure in `~/multi-ollama-server/`:
- `server.js` - Express backend application
- `package.json` - Node.js dependencies and scripts
- `public/index.html` - Complete frontend application
- `chat_history.db` - SQLite database for persistence
- `hosts.json` - Host configuration file
- `settings.yaml` - Application settings
- `uploads/` - Temporary file processing directory
- `uninstall.sh` - Service removal script